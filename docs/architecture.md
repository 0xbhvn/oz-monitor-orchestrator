# System Architecture

```mermaid
flowchart TB
    %% External Layer
    Clients["ğŸŒ <b>External Clients</b><br/>Web Apps â€¢ APIs â€¢ Scripts â€¢ Monitoring Tools"]
    
    %% API Layer
    subgraph APILayer["ğŸ” <b>Stellar Monitor Tenant Isolation</b>"]
        direction LR
        API1["ğŸ”‘ REST API<br/>JWT/API Key Auth"]
        API2["ğŸ‘¥ Tenant Management<br/>& RBAC"]
        API3["ğŸ“Š Resource Quotas<br/>& Rate Limiting"]
        API4["ğŸ“ Audit Logging<br/>& Compliance"]
    end
    
    %% Database
    subgraph PostgreSQL["ğŸ—„ï¸ <b>PostgreSQL Database</b>"]
        Tables["<b>Tables:</b><br/>tenants â€¢ tenant_users<br/>tenant_monitors â€¢ tenant_networks<br/>tenant_triggers â€¢ api_keys<br/>audit_logs"]
    end
    
    %% Orchestrator Components
    subgraph OrchestratorLayer["âš™ï¸ <b>OZ Monitor Orchestrator</b>"]
        direction TB
        LoadBalancer["ğŸ”„ <b>Load Balancer</b><br/>â€¢ Consistent Hashing<br/>â€¢ Dynamic Rebalancing"]
        WorkerPool["ğŸ‘· <b>Worker Pool Manager</b><br/>â€¢ 10-50 Workers<br/>â€¢ Auto-scaling<br/>â€¢ Health Checks"]
        BlockWatchers["ğŸ” <b>Shared Block Watchers</b><br/>â€¢ 1 per Network<br/>â€¢ Deduped RPC calls<br/>â€¢ Channel Broadcasting"]
        
        LoadBalancer --> WorkerPool
        WorkerPool --> BlockWatchers
    end
    
    %% Redis
    subgraph Redis["ğŸ’¾ <b>Redis Cache</b>"]
        RedisData["<b>Cached Data:</b><br/>â€¢ Blocks<br/>â€¢ Configurations<br/>â€¢ State Management"]
    end
    
    %% Core Monitor
    subgraph MonitorCore["ğŸ“¦ <b>OpenZeppelin Monitor Core</b>"]
        direction LR
        Core1["ğŸ”— <b>Multi-Blockchain</b><br/>EVM Chains<br/>Stellar"]
        Core2["ğŸ¯ <b>Core Features</b><br/>Filter Language<br/>Event Matching"]
        Core3["ğŸ“¢ <b>Notifications</b><br/>Slack â€¢ Discord<br/>Email â€¢ Webhooks"]
    end
    
    %% Blockchains
    Chains["â›“ï¸ <b>Blockchain Networks</b><br/>Stellar â€¢ Ethereum â€¢ Polygon â€¢ BSC â€¢ Arbitrum â€¢ Optimism"]
    
    %% Main Flow Connections
    Clients ==>|"HTTPS<br/>JWT/API Keys"| APILayer
    APILayer ==>|"Store/Query<br/>Configs"| PostgreSQL
    APILayer -.->|"Metrics &<br/>Monitoring"| OrchestratorLayer
    OrchestratorLayer <===>|"Read/Write<br/>Tenant Data"| PostgreSQL
    BlockWatchers <===>|"Cache<br/>Operations"| Redis
    OrchestratorLayer ==>|"Uses as<br/>Library"| MonitorCore
    MonitorCore ==>|"RPC<br/>Calls"| Chains
    
    %% Enhanced Styling
    classDef clientBox fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,color:#0d47a1
    classDef apiBox fill:#e8eaf6,stroke:#5e35b1,stroke-width:2px,color:#4527a0
    classDef dbBox fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#e65100
    classDef orchestratorBox fill:#f3e5f5,stroke:#8e24aa,stroke-width:2px,color:#6a1b9a
    classDef cacheBox fill:#ffebee,stroke:#e53935,stroke-width:2px,color:#c62828
    classDef coreBox fill:#e8f5e9,stroke:#43a047,stroke-width:2px,color:#2e7d32
    classDef chainBox fill:#fce4ec,stroke:#d81b60,stroke-width:2px,color:#ad1457
    
    class Clients clientBox
    class APILayer apiBox
    class PostgreSQL dbBox
    class OrchestratorLayer orchestratorBox
    class Redis cacheBox
    class MonitorCore coreBox
    class Chains chainBox
```

## Key Architectural Principles

  1. Layered Architecture
    - API Layer: Stellar Monitor Tenant Isolation handles all external interactions
    - Orchestration Layer: OZ Monitor Orchestrator manages distributed processing
    - Core Engine: OpenZeppelin Monitor provides blockchain monitoring logic
  2. Data Flow
    - Tenant configurations stored in PostgreSQL via API layer
    - Orchestrator reads configs and distributes to workers
    - Workers use OpenZeppelin Monitor library for processing
    - Results flow back through the layers
  3. Scaling Strategy
    - Vertical Separation: Each layer scales independently
    - Horizontal Scaling: Worker pool scales 10-50 instances
    - Resource Efficiency: Single block fetch serves all tenants
  4. Multi-Tenancy Implementation
    - Database Isolation: All queries filtered by tenant_id
    - Resource Quotas: Enforced at API layer
    - Worker Distribution: Consistent hashing for tenant affinity

## Integration Mechanisms

  1. Configuration Propagation
  API â†’ PostgreSQL â†’ Orchestrator â†’ Worker â†’ OZ Monitor
  2. Block Processing Pipeline
  Blockchain â†’ Shared Watcher â†’ Redis â†’ Workers â†’ Tenant Filters
  3. Authentication Flow
  Client â†’ JWT/API Key â†’ Tenant Context â†’ All Operations

## Performance Characteristics

- O(1) Block Fetching: One fetch per block regardless of tenant count
- O(n) Filter Processing: Distributed across worker pool
- Sub-second Latency: Redis caching for active data
- Linear Scalability: Add workers to handle more tenants

## Security & Isolation

- Complete Tenant Isolation: Database-level filtering
- Role-Based Access Control: Hierarchical permissions
- API Key Scoping: Fine-grained access control
- Audit Trail: All actions logged with context

## Configuration Management & Caching

### Configuration Flow

1. **Storage**: All tenant configurations stored in PostgreSQL
   - Monitor definitions with filter rules
   - Network configurations and RPC endpoints
   - Trigger definitions and scripts

2. **Loading**: Workers load configurations on startup
   - Tenant-aware repositories query PostgreSQL
   - Configurations cached in memory (`DashMap` structures)
   - No automatic refresh mechanism currently

3. **Caching Behavior**:
   - In-memory cache persists for worker lifetime
   - No TTL or invalidation mechanism
   - Configuration changes require worker restart

4. **Update Propagation**:
   - Database updates don't trigger cache refresh
   - Workers continue with cached configurations
   - See [Configuration Updates Documentation](./configuration-updates.md) for details

### Cache Architecture

```bash
PostgreSQL (Source of Truth)
    â†“ (Load on startup)
Worker Memory Cache
    - monitor_cache: DashMap<Uuid, HashMap<String, Monitor>>
    - contract_spec_cache: DashMap<String, ContractSpec>
    - trigger_script_cache: DashMap<String, String>
```

## Deployment Architecture

- Kubernetes Native: Designed for K8s deployment
- Auto-scaling: HPA based on CPU, memory, tenant count
- High Availability: Multiple replicas with pod disruption budgets
- Observability: Prometheus metrics at each layer
